{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "latent_dims = 10\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 2e-4\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((400,400)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do you have CUDA available?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the image data folder and\n",
    "# transform function to the datasets\n",
    "# .imagefolder function\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "### Landscapes ###\n",
    "landscape_training = datasets.ImageFolder(PATH +\"\\landscapes\", transform=img_transform)\n",
    "ltrain_dataloader = torch.utils.data.DataLoader(landscape_training, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "landscape_test = datasets.ImageFolder(PATH + \"\\landscapes\", transform=img_transform)\n",
    "ltest_dataloader = torch.utils.data.DataLoader(landscape_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "### Paintings ###\n",
    "painting_training = datasets.ImageFolder(PATH + \"\\paintings\", transform=img_transform)\n",
    "ptrain_dataloader = torch.utils.data.DataLoader(painting_training, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "painting_test = datasets.ImageFolder(PATH +\"\\paintings\", transform=img_transform)\n",
    "ptest_dataloader = torch.utils.data.DataLoader(painting_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "# from numpy import imag\n",
    "# # iter function iterates through all the\n",
    "# # images and labels and stores in two variables\n",
    "# images, labels = next(iter(ltrain_dataloader))\n",
    "  \n",
    "# # print the total no of samples\n",
    "# print('Number of samples: ', len(images))\n",
    "# image = images[2]  # load 3rd sample\n",
    "  \n",
    "# cv2.imshow('sample', image)\n",
    "# cv2.waitKey(0) # waits until a key is pressed\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ecsan\\anaconda3\\envs\\ML\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ltrain_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# iter function iterates through all the\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# images and labels and stores in two variables\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(ltrain_dataloader))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print the total no of samples\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of samples: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ltrain_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# iter function iterates through all the\n",
    "# images and labels and stores in two variables\n",
    "images, labels = next(iter(ltrain_dataloader))\n",
    "  \n",
    "# print the total no of samples\n",
    "print('Number of samples: ', len(images))\n",
    "image = images[2]  # load sample\n",
    "print(np.shape(image))\n",
    "\n",
    "# visualize the image\n",
    "plt.axis(\"off\")\n",
    "image=(image+1)/2\n",
    "plt.imshow(image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO data (path)\n",
    "dataset_name = 'gan-getting-started'\n",
    "root = '../input/'+dataset_name\n",
    "\n",
    "# data (img)\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "\n",
    "# training\n",
    "epoch = 0 # epoch to start training from\n",
    "n_epochs = 5 # number of epochs of training\n",
    "batch_size = 1 # size of the batches\n",
    "lr = 0.0002 # adam : learning rate\n",
    "b1 = 0.5 # adam : decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam : decay of first order momentum of gradient\n",
    "decay_epoch = 3 # suggested default : 100 (suggested 'n_epochs' is 200)\n",
    "                 # epoch from which to start lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_block):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        \n",
    "        channels = input_shape[0]\n",
    "        \n",
    "        # Initial Convolution Block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_block):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "            \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            \n",
    "        # Output Layer\n",
    "        model += [nn.ReflectionPad2d(channels),\n",
    "                  nn.Conv2d(out_features, channels, 7),\n",
    "                  nn.Tanh()\n",
    "                 ]\n",
    "        \n",
    "        # Unpacking\n",
    "        self.model = nn.Sequential(*model) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        channels, height, width = input_shape\n",
    "        \n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height//2**4, width//2**4)\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128,256),\n",
    "            *discriminator_block(256,512),\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width) # (3,256,256)\n",
    "n_residual_blocks = 9 # suggested default, number of residual blocks in generator\n",
    "\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availibility: False\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(f'CUDA Availibility: {cuda}')\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    \n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): ZeroPad2d((1, 0, 1, 0))\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)\n",
    "\n",
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_weights_init_normal(m):\n",
    "    classname =  m.__class__.__name__\n",
    "    print(classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "InstanceNorm2d\n",
      "Sequential\n",
      "ResidualBlock\n",
      "Upsample\n",
      "Conv2d\n",
      "ReLU\n",
      "Upsample\n",
      "Conv2d\n",
      "ReLU\n",
      "ReflectionPad2d\n",
      "Conv2d\n",
      "Tanh\n",
      "Sequential\n",
      "GeneratorResNet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratorResNet(\n",
       "  (model): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (19): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (20): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (23): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.apply(temp_weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# lr = 0.0002\n",
    "# b1 = 0.5\n",
    "# b2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "        \n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch+self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 10\n",
    "# epoch = 0\n",
    "# decay_epoch = 5\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# lr = 0.0002\n",
    "# b1 = 0.5\n",
    "# b2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ecsan\\AppData\\Local\\Temp\\ipykernel_19072\\345895398.py:5: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n",
      "c:\\Users\\ecsan\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(image):\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[:500])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[:500])\n",
    "        elif self.mode == 'test':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[500:])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[500:]) # was 250:301\n",
    "\n",
    "    def  __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "        \n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[np.random.randint(0, len(self.files_B)-1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "        if image_A.mode != 'RGB':\n",
    "            image_A = to_rgb(image_A)\n",
    "        if image_B.mode != 'RGB':\n",
    "            image_B = to_rgb(image_B)\n",
    "            \n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {'A':item_A, 'B':item_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=1, # 1\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu # 3\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True, mode='test'),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images():\n",
    "    \"\"\"show a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = imgs['A'].type(Tensor) # A : monet\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    real_B = imgs['B'].type(Tensor) # B : photo\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis    \n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    plt.imshow(image_grid.cpu().permute(1,2,0))\n",
    "    plt.title('Real A vs Fake B | Real B vs Fake A')\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imgs = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB.eval() # test mode \n",
    "G_BA.eval() # test mode\n",
    "print(temp_imgs['A'].shape)\n",
    "print(temp_imgs['B'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_real_A = temp_imgs['A'].type(Tensor) # A : monet\n",
    "temp_fake_B = G_AB(temp_real_A).detach()\n",
    "temp_real_B = temp_imgs['B'].type(Tensor) # B : photo\n",
    "temp_fake_A = G_BA(temp_real_B).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_real_A.shape)\n",
    "print(temp_fake_B.shape)\n",
    "print(temp_real_B.shape)\n",
    "print(temp_fake_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_real_A = make_grid(temp_real_A, nrow=5, normalize=True)\n",
    "temp_real_B = make_grid(temp_real_B, nrow=5, normalize=True)\n",
    "temp_fake_A = make_grid(temp_fake_A, nrow=5, normalize=True)\n",
    "temp_fake_B = make_grid(temp_fake_B, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(temp_real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_real_A.cpu().permute(1,2,0))\n",
    "plt.title('Real A')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_real_A.shape)\n",
    "print(temp_fake_B.shape)\n",
    "print(temp_real_B.shape)\n",
    "print(temp_fake_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_grid = torch.cat((temp_real_A, temp_fake_A, temp_real_B, temp_fake_B), 1)\n",
    "print(temp_image_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_grid.cpu().permute(1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_image_grid.cpu().permute(1,2,0))\n",
    "plt.title('Real A | Fake B | Real B | Fake A ')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image_grid = torch.cat((temp_real_A, temp_fake_A, temp_real_B, temp_fake_B), 1)\n",
    "print(temp_image_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        # Set model input\n",
    "        real_A = batch['A'].type(Tensor)\n",
    "        real_B = batch['B'].type(Tensor)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        \n",
    "# -----------------\n",
    "# Train Generators\n",
    "# -----------------\n",
    "        G_AB.train() # train mode\n",
    "        G_BA.train() # train mode\n",
    "        \n",
    "        optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)\n",
    "        \n",
    "        # Identity Loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.\n",
    "                                                             # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').\n",
    "        loss_identity = (loss_id_A + loss_id_B)/2\n",
    "        \n",
    "        # GAN Loss\n",
    "        fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'\n",
    "        \n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA)/2\n",
    "        \n",
    "        # Cycle Loss\n",
    "        recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        \n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B)/2\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "# -----------------\n",
    "# Train Discriminator A\n",
    "# -----------------\n",
    "        optimizer_D_A.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_A = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "# -----------------\n",
    "# Train Discriminator B\n",
    "# -----------------\n",
    "        optimizer_D_B.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_B = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_D = (loss_D_A + loss_D_B)/2\n",
    "    \n",
    "# -----------------\n",
    "# Show Progress\n",
    "# -----------------\n",
    "        if (i+1) % 50 == 0:\n",
    "            sample_images()\n",
    "            print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n",
    "                    %(epoch+1,n_epochs,       # [Epoch -]\n",
    "                      i+1,len(dataloader),   # [Batch -]\n",
    "                      loss_D.item(),       # [D loss -]\n",
    "                      loss_G.item(),       # [G loss -]\n",
    "                      loss_GAN.item(),     # [adv -]\n",
    "                      loss_cycle.item(),   # [cycle -]\n",
    "                      loss_identity.item(),# [identity -]\n",
    "                     ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print('iter : {}  A.size : {}'.format(i,batch['A'].size()))\n",
    "    print('iter : {}  B.size : {}'.format(i,batch['B'].size()))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A = batch['A'].type(Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = temp_A.squeeze()\n",
    "temp_img = temp_img.cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_A.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A.size(0) # batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor(np.ones((temp_A.size(0), *D_A.output_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor(np.ones((temp_A.size(0), *D_A.output_shape))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, d=128):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "#         self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "#         self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "#         self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "#         self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "#         self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "#         self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "#         self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # x = F.relu(self.deconv1(input))\n",
    "#         x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "#         x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "#         x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "#         x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "#         x = torch.tanh(self.deconv5(x))\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, d=128):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
    "#         self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "#         self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "#         self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "#         self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "#         self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "#         self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "#         self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "#         x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "#         x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "#         x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "#         x = torch.sigmoid(self.conv5(x))\n",
    "\n",
    "#         return x\n",
    "\n",
    "    \n",
    "# generator = Generator()\n",
    "# discriminator = Discriminator()\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if True and torch.cuda.is_available() else \"cpu\")\n",
    "# generator = generator.to(device)\n",
    "# discriminator = discriminator.to(device)\n",
    "\n",
    "# num_params_gen = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "# num_params_disc = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "# print('Number of parameters for generator: %d and discriminator: %d' % (num_params_gen, num_params_disc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GAN training can be unstable. In this case, the strong momentum\n",
    "# # for the gradient prevents convergence. One possible explanation is that the\n",
    "# # strong momentum does not allow the two players in the adversarial game to react\n",
    "# # to each other quickly enough. Decreasing beta1 (the exponential decay for the\n",
    "# # gradient moving average in [0,1], lower is faster decay) from the default 0.9\n",
    "# # to 0.5 allows for quicker reactions.\n",
    "# gen_optimizer = torch.optim.Adam(params=generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "# disc_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# # set to training mode\n",
    "# generator.train()\n",
    "# discriminator.train()\n",
    "\n",
    "# gen_loss_avg = []\n",
    "# disc_loss_avg = []\n",
    "\n",
    "# print('Training ...')\n",
    "# for epoch in range(num_epochs):\n",
    "#     gen_loss_avg.append(0)\n",
    "#     disc_loss_avg.append(0)\n",
    "#     num_batches = 0\n",
    "    \n",
    "#     for image_batch, _ in ltrain_dataloader:\n",
    "        \n",
    "#         # get dataset image and create real and fake labels for use in the loss\n",
    "#         image_batch = image_batch.to(device)\n",
    "#         label_real = torch.ones(image_batch.size(0), device=device)\n",
    "#         label_fake = torch.zeros(image_batch.size(0), device=device)\n",
    "\n",
    "#         # generate a batch of images from samples of the latent prior\n",
    "#         latent = torch.randn(image_batch.size(0), 3, 400 , 400, device=device)\n",
    "#         fake_image_batch = generator(latent)\n",
    "        \n",
    "#         # train discriminator to correctly classify real and fake\n",
    "#         # (detach the computation graph of the generator and the discriminator,\n",
    "#         # so that gradients are not backpropagated into the generator)\n",
    "#         real_pred = discriminator(image_batch).squeeze()\n",
    "#         fake_pred = discriminator(fake_image_batch.detach()).squeeze()\n",
    "#         disc_loss = 0.5 * (\n",
    "#             F.binary_cross_entropy(real_pred, label_real) +\n",
    "#             F.binary_cross_entropy(fake_pred, label_fake))\n",
    "        \n",
    "#         disc_optimizer.zero_grad()\n",
    "#         disc_loss.backward()\n",
    "#         disc_optimizer.step()\n",
    "        \n",
    "#         # train generator to output an image that is classified as real\n",
    "#         fake_pred = discriminator(fake_image_batch).squeeze()\n",
    "#         gen_loss = F.binary_cross_entropy(fake_pred, label_real)\n",
    "        \n",
    "#         gen_optimizer.zero_grad()\n",
    "#         gen_loss.backward()\n",
    "#         gen_optimizer.step()\n",
    "        \n",
    "#         gen_loss_avg[-1] += gen_loss.item()\n",
    "#         disc_loss_avg[-1] += disc_loss.item()\n",
    "#         num_batches += 1\n",
    "        \n",
    "#     gen_loss_avg[-1] /= num_batches\n",
    "#     disc_loss_avg[-1] /= num_batches\n",
    "#     print('Epoch [%d / %d] average loss generator vs. discrim.: %f vs. %f' %\n",
    "#           (epoch+1, num_epochs, gen_loss_avg[-1], disc_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c4b7abad336165b8b52684b07a1890cb56f9af1b6a4eff176a255c515c54d9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
